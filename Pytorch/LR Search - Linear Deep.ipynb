{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MLP\n",
    "from data import gen_rnd_ds\n",
    "from losses import LinearClassification\n",
    "from block_analysis import block_hessian, curvature_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ds_loss():    \n",
    "    model =  MLP(inp_dim, hid_dim, out_dim, nlayer, bias, mode).cuda(device)\n",
    "    ds = gen_rnd_ds(inp_dim, inp_mean, inp_var, \n",
    "                   out_dim, nsamp, device)\n",
    "    loss_fn = LinearClassification(out_dim)\n",
    "    return model, ds, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(lr, delta, h, H):\n",
    "    rel = abs((h.item() - H.sum().item()) / min(abs(H.sum().item()), abs(h.item())))\n",
    "    ratio = h.item() / H.sum().item()\n",
    "    print(f\"LR {lr:.2E} \\t || Delta={delta:.2E}\\t ||Error={rel:.2E}  \\t|| hoe={h.item()} \\t|| H={H.sum().item()}\\t||ratio={ratio}\")\n",
    "\n",
    "def lr_range(model, ds, loss_fn, start=-8, stop=8, step=1, log_scale=False):\n",
    "    for lr in range(start, stop, step):\n",
    "        if log_scale:\n",
    "            lr = 10**lr\n",
    "        H = block_hessian(model, ds, loss_fn, lr)\n",
    "        delta, h = curvature_effects(model, ds, loss_fn, lr)\n",
    "        pp(lr, delta, h, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "mode = \"linear\"\n",
    "bias = False\n",
    "nlayer = 3\n",
    "inp_dim = 10 \n",
    "out_dim = 10\n",
    "hid_dim = 100\n",
    "\n",
    "# Data parameters\n",
    "nsamp = 100\n",
    "inp_mean = 0\n",
    "inp_var = 1\n",
    "\n",
    "# Others\n",
    "device = 0\n",
    "lr = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import pair_indexes, init_hessian, \\\n",
    "                  clone_model, get_param, \\\n",
    "                  get_delta_params, dot_product\n",
    "\n",
    "def update_params(params, deltas, lr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for param, delta in zip(params, deltas):\n",
    "        param.data.add_(-lr, delta)\n",
    "    \n",
    "def higher_orders(loss_t, loss_t1, lr, grad, delta):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    first_order = dot_product(grad, delta)\n",
    "    return (loss_t - loss_t1 - lr*first_order) / (2*lr**2)\n",
    "\n",
    "def eval_loss(model, ds, loss_fn, compute_grad=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for x,y in ds:\n",
    "        if compute_grad:\n",
    "            loss_ = loss_fn(model(x),y)\n",
    "            loss_.backward()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                loss_ = loss_fn(model(x),y)\n",
    "        loss+=loss_.item()\n",
    "    return loss\n",
    "\n",
    "def block_hessian_off_diag(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    base_model = clone_model(model)\n",
    "    H = init_hessian(base_model)\n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(base_model, ds, loss_fn, True)\n",
    "    grads = {i:x.grad for i,x in enumerate(base_model.parameters())}\n",
    "    \n",
    "    for i,j in pair_indexes(base_model):\n",
    "        # Copy the full model for now.\n",
    "        # If this is too slow, we should make update_params\n",
    "        # a context manager instead.\n",
    "        model = clone_model(base_model)\n",
    "        pair  = (get_param(model, i), get_param(model, j))\n",
    "        grad  = (grads[i].clone(), grads[j].clone())\n",
    "        # Compute delta_theta (=normalized gradient vector for now)\n",
    "        # But we need to consider other training algo (momentum, etc.)\n",
    "        delta = get_delta_params(model, grad) \n",
    "        # Possible context manager\n",
    "        update_params(pair, delta, lr) \n",
    "        loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "        \n",
    "        h = higher_orders(loss_t, loss_t1, lr, grad, delta)\n",
    "        H[i,j] = H[j,i] = h/2\n",
    "    return H\n",
    "\n",
    "def block_hessian_diag(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    base_model = clone_model(model) \n",
    "    diagonal = []\n",
    "    \n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(base_model, ds, loss_fn, True)\n",
    "    grads = {i:x.grad for i,x in enumerate(base_model.parameters())}\n",
    "\n",
    "    for i,_ in enumerate(base_model.parameters()):\n",
    "        model = clone_model(base_model)        \n",
    "        pair  = (get_param(model, i), )\n",
    "        grad  = (grads[i].clone(),)\n",
    "        delta = get_delta_params(model, grad)\n",
    "        \n",
    "        update_params(pair, delta, lr)\n",
    "        loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "        h = higher_orders(loss_t, loss_t1, lr, grad, delta)\n",
    "        diagonal.append(h)\n",
    "        \n",
    "    return torch.cat(list(map(lambda x:x.view((1,)), diagonal)))\n",
    "\n",
    "def _merge_blocks(H, d):\n",
    "    \"\"\"\n",
    "        Substract H_{ij} = H_{ij}-d_{i}-d_{j}\n",
    "        Set H_{ii} = d_{i}\n",
    "    \"\"\"\n",
    "    D = -(d.view(1,-1) + d.view(-1,1)) \n",
    "    D[range(D.shape[0]), range(D.shape[0])]=d\n",
    "    return H + D\n",
    "\n",
    "def block_hessian(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "        Missing merge_DH(D, H)\n",
    "    \"\"\"\n",
    "    d = block_hessian_diag(model, ds, loss_fn, lr)\n",
    "    H = block_hessian_off_diag(model, ds, loss_fn, lr)\n",
    "    return _merge_blocks(H, d)\n",
    "\n",
    "def curvature_effects(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "    model = clone_model(model) \n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(model, ds, loss_fn, True)\n",
    "    grads = [x.grad for x in model.parameters()]\n",
    "    delta = get_delta_params(model, grads)\n",
    "    params = list(model.parameters())\n",
    "    \n",
    "    update_params(params, delta, lr)\n",
    "    loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "    return loss_t - loss_t1, higher_orders(loss_t, loss_t1, lr, grads, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H = block_hessian(model, ds, loss_fn, lr)\n",
    "#h = curvature_effects(model, ds, loss_fn, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import pair_indexes, init_hessian, \\\n",
    "                  clone_model, get_param, \\\n",
    "                  get_delta_params, dot_product\n",
    "\n",
    "def update_params(params, deltas, lr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for param, delta in zip(params, deltas):\n",
    "        param.data.add_(-lr, delta)\n",
    "    \n",
    "def eval_loss(model, ds, loss_fn, compute_grad=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for x,y in ds:\n",
    "        if compute_grad:\n",
    "            loss_ = loss_fn(model(x),y)\n",
    "            loss_.backward()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                loss_ = loss_fn(model(x),y)\n",
    "        loss+=loss_.item()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_orders(loss_t, loss_t1, lr, grad, delta):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    first_order = dot_product(grad, delta)\n",
    "    return (loss_t - loss_t1 - lr*first_order) / (lr**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def block_hessian_off_diag(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    base_model = clone_model(model)\n",
    "    H = init_hessian(base_model)\n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(base_model, ds, loss_fn, True)\n",
    "    grads = {i:x.grad for i,x in enumerate(base_model.parameters())}\n",
    "    \n",
    "    for i,j in pair_indexes(base_model):\n",
    "        # Copy the full model for now.\n",
    "        # If this is too slow, we should make update_params\n",
    "        # a context manager instead.\n",
    "        model = clone_model(base_model)\n",
    "        pair  = (get_param(model, i), get_param(model, j))\n",
    "        grad  = (grads[i].clone(), grads[j].clone())\n",
    "        # Compute delta_theta (=normalized gradient vector for now)\n",
    "        # But we need to consider other training algo (momentum, etc.)\n",
    "        delta = grad#get_delta_params(model, grad) \n",
    "        # Possible context manager\n",
    "        update_params(pair, delta, lr) \n",
    "        loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "        \n",
    "        h = higher_orders(loss_t, loss_t1, lr, grad, delta)\n",
    "        H[i,j] = H[j,i] = h\n",
    "    return H\n",
    "\n",
    "def block_hessian_diag(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    base_model = clone_model(model) \n",
    "    diagonal = []\n",
    "    \n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(base_model, ds, loss_fn, True)\n",
    "    grads = {i:x.grad for i,x in enumerate(base_model.parameters())}\n",
    "\n",
    "    for i,_ in enumerate(base_model.parameters()):\n",
    "        model = clone_model(base_model)        \n",
    "        pair  = (get_param(model, i), )\n",
    "        grad  = (grads[i].clone(),)\n",
    "        delta = grad#get_delta_params(model, grad)\n",
    "        \n",
    "        update_params(pair, delta, lr)\n",
    "        loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "        h = higher_orders(loss_t, loss_t1, lr, grad, delta)\n",
    "        diagonal.append(h)\n",
    "        \n",
    "    return torch.cat(list(map(lambda x:x.view((1,)), diagonal)))\n",
    "\n",
    "def _merge_blocks(H, d):\n",
    "    \"\"\"\n",
    "        Substract H_{ij} = (H_{ij}-d_{i}-d_{j})/2\n",
    "        Set H_{ii} = d_{i}\n",
    "    \"\"\"\n",
    "    D = -(d.view(1,-1) + d.view(-1,1))\n",
    "    H = (H+D)/2\n",
    "    H[range(H.shape[0]), range(H.shape[0])]=d\n",
    "    return H \n",
    "\n",
    "def block_hessian(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "        Missing merge_DH(D, H)\n",
    "    \"\"\"\n",
    "    d = block_hessian_diag(model, ds, loss_fn, lr)\n",
    "    H = block_hessian_off_diag(model, ds, loss_fn, lr)\n",
    "    return _merge_blocks(H, d)\n",
    "\n",
    "def curvature_effects(model, ds, loss_fn, lr):\n",
    "    \"\"\"\n",
    "        Returns O(lr) terms of the Taylor expansion:\n",
    "        hoe = L(theta_t1) - L(theta_t) + lr * (delta_theta * grad_theta)\n",
    "    \"\"\"    \n",
    "    model = clone_model(model) \n",
    "    # Get loss(t) and gradients\n",
    "    loss_t = eval_loss(model, ds, loss_fn, True)\n",
    "    grads = [x.grad for x in model.parameters()]\n",
    "    delta = grads#get_delta_params(model, grads)\n",
    "    params = list(model.parameters())\n",
    "    \n",
    "    update_params(params, delta, lr)\n",
    "    loss_t1 = eval_loss(model, ds, loss_fn, False)\n",
    "    return loss_t - loss_t1, higher_orders(loss_t, loss_t1, lr, grads, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 1.00E-08 \t || Delta=0.00E+00\t ||Error=6.76E-08  \t|| hoe=-115494.40625 \t|| H=-115494.3984375\t||ratio=1.0000000676439733\n",
      "LR 1.00E-07 \t || Delta=0.00E+00\t ||Error=8.46E-08  \t|| hoe=-11549.4404296875 \t|| H=-11549.439453125\t||ratio=1.0000000845549695\n",
      "LR 1.00E-06 \t || Delta=5.82E-10\t ||Error=0.00E+00  \t|| hoe=-572.867431640625 \t|| H=-572.867431640625\t||ratio=1.0\n",
      "LR 1.00E-05 \t || Delta=1.13E-08\t ||Error=9.05E-01  \t|| hoe=-2.571542978286743 \t|| H=-4.899842739105225\t||ratio=0.5248215331001298\n",
      "LR 1.00E-04 \t || Delta=1.16E-07\t ||Error=5.23E-01  \t|| hoe=0.03388436138629913 \t|| H=0.02224309928715229\t||ratio=1.5233651097296\n",
      "LR 1.00E-03 \t || Delta=1.16E-06\t ||Error=2.71E+00  \t|| hoe=0.00012869350030086935 \t|| H=-0.00022040330804884434\t||ratio=-0.5839000396144196\n",
      "LR 1.00E-02 \t || Delta=1.15E-05\t ||Error=9.53E-01  \t|| hoe=1.2369127944111824e-06 \t|| H=2.4158453015843406e-06\t||ratio=0.512\n",
      "LR 1.00E-01 \t || Delta=1.15E-04\t ||Error=1.05E-02  \t|| hoe=-2.1122104953974485e-06 \t|| H=-2.1344476408557966e-06\t||ratio=0.9895817798325415\n",
      "LR 1.00E+00 \t || Delta=1.15E-03\t ||Error=2.64E-04  \t|| hoe=-2.1470477804541588e-06 \t|| H=-2.147615305148065e-06\t||ratio=0.9997357419214951\n",
      "LR 1.00E+01 \t || Delta=1.13E-02\t ||Error=2.77E-03  \t|| hoe=-2.1418090909719467e-06 \t|| H=-2.147737404811778e-06\t||ratio=0.9972397399111503\n",
      "LR 1.00E+02 \t || Delta=9.46E-02\t ||Error=2.84E-02  \t|| hoe=-2.0883933302684454e-06 \t|| H=-2.1477421796589624e-06\t||ratio=0.9723668650955392\n",
      "LR 1.00E+03 \t || Delta=-3.99E-01\t ||Error=3.82E-01  \t|| hoe=-1.5542368601018097e-06 \t|| H=-2.1477426344063133e-06\t||ratio=0.7236606636211035\n",
      "LR 1.00E+04 \t || Delta=3.90E+02\t ||Error=2.76E+00  \t|| hoe=3.787312834901968e-06 \t|| H=-2.147742407032638e-06\t||ratio=-1.7633924918093842\n",
      "LR 1.00E+05 \t || Delta=5.72E+05\t ||Error=2.76E+01  \t|| hoe=5.720280023524538e-05 \t|| H=-2.147741952285287e-06\t||ratio=-26.63392600511398\n",
      "LR 1.00E+06 \t || Delta=5.91E+08\t ||Error=2.76E+02  \t|| hoe=0.0005913577624596655 \t|| H=-2.1477426344063133e-06\t||ratio=-275.339211033137\n",
      "LR 1.00E+07 \t || Delta=5.93E+11\t ||Error=2.76E+03  \t|| hoe=0.005932907108217478 \t|| H=-2.1477421796589624e-06\t||ratio=-2762.3926020578306\n"
     ]
    }
   ],
   "source": [
    "model, ds, loss_fn = get_model_ds_loss()\n",
    "lr_range(model, ds, loss_fn, -8, 8, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0281e-06, device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_hessian_off_diag(model, ds, loss_fn, lr).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.0930e-04, 2.4471e-07, 7.8022e-07],\n",
       "         [2.4471e-07, 3.6775e-03, 3.1083e-06],\n",
       "         [7.8022e-07, 3.1083e-06, 4.3624e-03]], device='cuda:0'),\n",
       " tensor(0.0084, device='cuda:0'))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = block_hessian(model, ds, loss_fn, 0.1)\n",
    "H, H.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33b5f78518>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOMElEQVR4nO3df6zddX3H8eeL2wIxoPyoG00pIlnjxtwW8QZRF9NMTZAYukSW4BIFo7nRSaaLJkNdMDFZpv7hMqeRVCXCYpCIRq9LjcEBw2WBUUmhlAa5kCzctBFtXZGosOJ7f9wv29nl3N7bz/nec07x+UhOzvfH53w/bz6EVz/fXzRVhSQdr5MmXYCkE5PhIamJ4SGpieEhqYnhIamJ4SGpyUjhkeSsJLcleaT7PnOFds8m2dN95kfpU9J0yCjPeST5NHC4qj6Z5FrgzKr66yHtnqqq00aoU9KUGTU8Hga2V9XBJJuBO6vqFUPaGR7SC8yo4fFfVXXGwPrPqup5py5JjgJ7gKPAJ6vqWyscbw6YA5hh5tUv4sXNtb3QZcPMpEuYetsu/PmkS5h6P3zg6Z9W1UtbfrthtQZJvg+cM2TXx46jn/Oq6kCSC4Dbk+ytqkeXN6qqncBOgBfnrHrNSW86ji5+s8ycMfTykgbs+t7tky5h6s1sXvjP1t+uGh5VteJ/wUl+nGTzwGnLEysc40D3/ViSO4FXAc8LD0knjlFv1c4DV3XLVwHfXt4gyZlJTumWNwGvBx4asV9JEzZqeHwSeHOSR4A3d+skmU3ypa7N7wG7k9wP3MHSNQ/DQzrBrXracixVdQh445Dtu4H3dMv/DvzBKP1Imj4+YSqpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIalJL+GR5NIkDydZSHLtkP2nJLml239PkvP76FfS5IwcHklmgM8DbwEuBN6e5MJlzd4N/Kyqfgf4e+BTo/YrabL6mHlcDCxU1WNV9QzwNWDHsjY7gBu75VuBNyZJD31LmpA+wmML8PjA+mK3bWibqjoKHAHO7qFvSROyoYdjDJtBVEMbkswBcwCn8qLRK5O0bvqYeSwCWwfWzwUOrNQmyQbgJcDh5Qeqqp1VNVtVsxs5pYfSJK2XPsLjXmBbkpcnORm4Ephf1mYeuKpbvgK4vaqeN/OQdOIY+bSlqo4muQb4HjAD3FBV+5J8AthdVfPAl4F/SrLA0ozjylH7lTRZfVzzoKp2AbuWbbtuYPlXwJ/10Zek6eATppKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKa9BIeSS5N8nCShSTXDtl/dZKfJNnTfd7TR7+SJmfDqAdIMgN8HngzsAjcm2S+qh5a1vSWqrpm1P4kTYc+Zh4XAwtV9VhVPQN8DdjRw3ElTbGRZx7AFuDxgfVF4DVD2r0tyRuAHwF/VVWPL2+QZA6YAzj1pNOYOevMHsp7YXr20OFJlzD1tt159aRLOAH8TfMv+5h5ZMi2Wrb+HeD8qvpD4PvAjcMOVFU7q2q2qmZPPunUHkqTtF76CI9FYOvA+rnAgcEGVXWoqp7uVr8IvLqHfiVNUB/hcS+wLcnLk5wMXAnMDzZIsnlg9XJgfw/9Spqgka95VNXRJNcA3wNmgBuqal+STwC7q2oe+MsklwNHgcPA1aP2K2my+rhgSlXtAnYt23bdwPJHgI/00Zek6eATppKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6SmhgekpoYHpKaGB6Smhgekpr0Eh5JbkjyRJIHV9ifJJ9NspDkgSQX9dGvpMnpa+bxFeDSY+x/C7Ct+8wBX+ipX0kT0kt4VNVdwOFjNNkB3FRL7gbOSLK5j74lTca4rnlsAR4fWF/stv0/SeaS7E6y+5lf/2pMpUlqMa7wyJBt9bwNVTuraraqZk8+6dQxlCWp1bjCYxHYOrB+LnBgTH1LWgfjCo954J3dXZdLgCNVdXBMfUtaBxv6OEiSm4HtwKYki8DHgY0AVXU9sAu4DFgAfgG8q49+JU1OL+FRVW9fZX8B7++jL0nTwSdMJTUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNTE8JDUxPCQ1MTwkNeklPJLckOSJJA+usH97kiNJ9nSf6/roV9Lk9PIXXQNfAT4H3HSMNj+oqrf21J+kCetl5lFVdwGH+ziWpBNDXzOPtXhtkvuBA8CHq2rf8gZJ5oA5gPO2bGDX7tvHWN6JZdudV0+6hKl3wZ/vmXQJU++xEX47rgum9wEvq6o/Av4R+NawRlW1s6pmq2r2pWfPjKk0SS3GEh5V9WRVPdUt7wI2Jtk0jr4lrY+xhEeSc5KkW7646/fQOPqWtD56ueaR5GZgO7ApySLwcWAjQFVdD1wBvC/JUeCXwJVVVX30LWkyegmPqnr7Kvs/x9KtXEkvED5hKqmJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqcnI4ZFka5I7kuxPsi/JB4a0SZLPJllI8kCSi0btV9Jk9fEXXR8FPlRV9yU5Hfhhktuq6qGBNm8BtnWf1wBf6L4lnaBGnnlU1cGquq9b/jmwH9iyrNkO4KZacjdwRpLNo/YtaXJ6veaR5HzgVcA9y3ZtAR4fWF/k+QEj6QTSW3gkOQ34BvDBqnpy+e4hP6khx5hLsjvJ7p8cerav0iStg17CI8lGloLjq1X1zSFNFoGtA+vnAgeWN6qqnVU1W1WzLz17po/SJK2TPu62BPgysL+qPrNCs3ngnd1dl0uAI1V1cNS+JU1OH3dbXg+8A9ibZE+37aPAeQBVdT2wC7gMWAB+Abyrh34lTdDI4VFV/8bwaxqDbQp4/6h9SZoePmEqqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqYnhIamJ4SGpieEhqcnI4ZFka5I7kuxPsi/JB4a02Z7kSJI93ee6UfuVNFkbejjGUeBDVXVfktOBHya5raoeWtbuB1X11h76kzQFRp55VNXBqrqvW/45sB/YMupxJU23VFV/B0vOB+4CXllVTw5s3w58A1gEDgAfrqp9Q34/B8x1q68EHuytuH5sAn466SIGWM+xTVs9MH01vaKqTm/5YW/hkeQ04F+Bv62qby7b92Lg11X1VJLLgH+oqm2rHG93Vc32UlxPpq0m6zm2aasHpq+mUerp5W5Lko0szSy+ujw4AKrqyap6qlveBWxMsqmPviVNRh93WwJ8GdhfVZ9Zoc05XTuSXNz1e2jUviVNTh93W14PvAPYm2RPt+2jwHkAVXU9cAXwviRHgV8CV9bq50s7e6itb9NWk/Uc27TVA9NXU3M9vV4wlfSbwydMJTUxPCQ1mZrwSHJWktuSPNJ9n7lCu2cHHnOfX4c6Lk3ycJKFJNcO2X9Kklu6/fd0z7asqzXUdHWSnwyMy3vWsZYbkjyRZOgzOFny2a7WB5JctF61HEdNY3s9Yo2va4x1jNbtFZKqmooP8Gng2m75WuBTK7R7ah1rmAEeBS4ATgbuBy5c1uYvgOu75SuBW9Z5XNZS09XA58b07+kNwEXAgyvsvwz4LhDgEuCeKahpO/DPYxqfzcBF3fLpwI+G/Psa6xitsabjHqOpmXkAO4Abu+UbgT+dQA0XAwtV9VhVPQN8ratr0GCdtwJvfO429ARrGpuqugs4fIwmO4CbasndwBlJNk+4prGptb2uMdYxWmNNx22awuO3q+ogLP3DAr+1QrtTk+xOcneSvgNmC/D4wPoizx/k/21TVUeBI8DZPddxvDUBvK2bAt+aZOs61rOatdY7bq9Ncn+S7yb5/XF02J3Svgq4Z9muiY3RMWqC4xyjPp7zWLMk3wfOGbLrY8dxmPOq6kCSC4Dbk+ytqkf7qZBhM4jl97LX0qZPa+nvO8DNVfV0kveyNDP6k3Ws6VjGPT5rcR/wsvq/1yO+BRzz9YhRda9rfAP4YA285/Xc7iE/WfcxWqWm4x6jsc48qupNVfXKIZ9vAz9+burWfT+xwjEOdN+PAXeylKJ9WQQG/9Q+l6UX+Ya2SbIBeAnrO2VetaaqOlRVT3erXwRevY71rGYtYzhWNebXI1Z7XYMJjNF6vEIyTact88BV3fJVwLeXN0hyZpJTuuVNLD3duvz/GzKKe4FtSV6e5GSWLoguv6MzWOcVwO3VXXFaJ6vWtOx8+XKWzmknZR54Z3dH4RLgyHOno5Myztcjun6O+boGYx6jtdTUNEbjuAK9xivCZwP/AjzSfZ/VbZ8FvtQtvw7Yy9Idh73Au9ehjstYuhr9KPCxbtsngMu75VOBrwMLwH8AF4xhbFar6e+Afd243AH87jrWcjNwEPhvlv4EfTfwXuC93f4An+9q3QvMjmF8VqvpmoHxuRt43TrW8scsnYI8AOzpPpdNcozWWNNxj5GPp0tqMk2nLZJOIIaHpCaGh6QmhoekJoaHpCaGh6QmhoekJv8D3uwIamJMEv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, ds, loss_fn = get_model_ds_loss()\n",
    "lr = 1\n",
    "H = block_hessian(model, ds, loss_fn, lr)\n",
    "plt.imshow(H.abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.0036e-11, -5.5043e-09,  1.6940e-07],\n",
       "        [-5.5043e-09, -2.9104e-10, -4.3772e-08],\n",
       "        [ 1.6940e-07, -4.3772e-08, -1.7462e-10]], device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 1.00E+00 \t || Error=6.54E-01  \t|| h=0.0019954312592744827 \t|| H=0.003301077987998724\n",
      "LR 2.00E+00 \t || Error=5.72E-01  \t|| h=0.0020995535887777805 \t|| H=0.003301077987998724\n",
      "LR 3.00E+00 \t || Error=4.97E-01  \t|| h=0.0022055571898818016 \t|| H=0.0033010789193212986\n",
      "LR 4.00E+00 \t || Error=4.27E-01  \t|| h=0.0023134429939091206 \t|| H=0.0033010775223374367\n",
      "LR 5.00E+00 \t || Error=3.62E-01  \t|| h=0.0024232110008597374 \t|| H=0.0033010777551680803\n",
      "LR 6.00E+00 \t || Error=3.02E-01  \t|| h=0.0025348614435642958 \t|| H=0.0033010784536600113\n",
      "LR 7.00E+00 \t || Error=2.46E-01  \t|| h=0.0026483931578695774 \t|| H=0.003301077289506793\n",
      "LR 8.00E+00 \t || Error=1.94E-01  \t|| h=0.0027638075407594442 \t|| H=0.003301076591014862\n",
      "LR 9.00E+00 \t || Error=1.46E-01  \t|| h=0.002881104126572609 \t|| H=0.0033010777551680803\n",
      "LR 1.00E+01 \t || Error=1.00E-01  \t|| h=0.0030002829153090715 \t|| H=0.0033010775223374367\n",
      "LR 1.10E+01 \t || Error=5.76E-02  \t|| h=0.003121343906968832 \t|| H=0.0033010770566761494\n",
      "LR 1.20E+01 \t || Error=1.75E-02  \t|| h=0.0032442871015518904 \t|| H=0.0033010777551680803\n",
      "LR 1.30E+01 \t || Error=2.06E-02  \t|| h=0.003369111567735672 \t|| H=0.0033010770566761494\n",
      "LR 1.40E+01 \t || Error=5.90E-02  \t|| h=0.0034958196338266134 \t|| H=0.0033010768238455057\n",
      "LR 1.50E+01 \t || Error=9.79E-02  \t|| h=0.0036244092043489218 \t|| H=0.0033010775223374367\n",
      "LR 1.60E+01 \t || Error=1.37E-01  \t|| h=0.003754880279302597 \t|| H=0.0033010777551680803\n",
      "LR 1.70E+01 \t || Error=1.78E-01  \t|| h=0.003887234488502145 \t|| H=0.0033010775223374367\n",
      "LR 1.80E+01 \t || Error=2.18E-01  \t|| h=0.004021470434963703 \t|| H=0.0033010775223374367\n",
      "LR 1.90E+01 \t || Error=2.59E-01  \t|| h=0.004157587420195341 \t|| H=0.0033010777551680803\n"
     ]
    }
   ],
   "source": [
    "lr_range(model, ds, loss_fn, 1, 20, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 637691.3125 1571090.625\n",
      "1e-07 -29363.134765625 -5872.3935546875\n",
      "1e-06 -258.7601318359375 -121.57786560058594\n",
      "1e-05 0.8995471000671387 1.8121325969696045\n",
      "0.0001 0.008458300493657589 0.006585310213267803\n",
      "0.001 0.00014551915228366852 0.00030877345125190914\n",
      "0.01 0.00042680767364799976 0.00042782630771398544\n",
      "0.1 0.00042814062908291817 0.00042815518099814653\n",
      "1 0.00042816251516342163 0.00042816437780857086\n",
      "10 0.00042816190398298204 0.00042816210770979524\n",
      "100 0.00042816175846382976 0.0004281617875676602\n",
      "1000 0.00042816175846382976 0.00042816175846382976\n",
      "10000 0.0004281617875676602 0.0004281617875676602\n",
      "100000 0.00042816181667149067 0.00042816181667149067\n",
      "1000000 0.0004281617875676602 0.0004281617875676602\n",
      "10000000 0.00042816175846382976 0.00042816175846382976\n"
     ]
    }
   ],
   "source": [
    "def func():\n",
    "    for log_lr in range(-8, 8): \n",
    "        lr = 10**log_lr\n",
    "        H = block_hessian(model, ds, loss_fn, lr)\n",
    "        h = curvature_effects(model, ds, loss_fn, lr)\n",
    "        print(lr, h.item(), H.sum().item())\n",
    "        \n",
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan nan\n",
      "1 0.039547525346279144 0.0537184402346611\n",
      "2 0.04295935854315758 0.05371849238872528\n",
      "3 0.04637118801474571 0.05371849983930588\n",
      "4 0.049783021211624146 0.05371849238872528\n",
      "5 0.05319485068321228 0.053718503564596176\n",
      "6 0.05660667642951012 0.053718507289886475\n",
      "7 0.06001850962638855 0.053718503564596176\n",
      "8 0.06343034654855728 0.05371851101517677\n",
      "9 0.06684217602014542 0.05371851474046707\n"
     ]
    }
   ],
   "source": [
    "for lr in range(10): \n",
    "    H = block_hessian(model, ds, loss_fn, lr)\n",
    "    h = curvature_effects(model, ds, loss_fn, lr)\n",
    "    print(lr, h.item(), H.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan nan\n",
      "1 0.0027682632207870483 0.005076628178358078\n",
      "2 0.00315653532743454 0.0050767287611961365\n",
      "3 0.003544833976775408 0.005076732020825148\n",
      "4 0.003933116793632507 0.005076744593679905\n",
      "5 0.004321407992392778 0.005076759494841099\n",
      "6 0.004709700588136911 0.005076758563518524\n",
      "7 0.005097983870655298 0.00507675064727664\n",
      "8 0.005486276000738144 0.0050767529755830765\n",
      "9 0.00587456626817584 0.005076766014099121\n"
     ]
    }
   ],
   "source": [
    "for lr in range(10): \n",
    "    H = block_hessian(model, ds, loss_fn, lr)\n",
    "    h = curvature_effects(model, ds, loss_fn, lr)\n",
    "    print(lr, h.item(), H.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
